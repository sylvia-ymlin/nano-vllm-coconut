# nano-vLLM Optimization: RMSNorm+Linear Fusion Research

> A research project implementing and optimizing the nano-vLLM lightweight inference framework with Triton-based kernel fusion.

## ğŸ“‹ Project Overview

This project reproduces the **nano-vLLM** lightweight LLM inference framework and implements a **RMSNorm+Linear operator fusion** using Triton to achieve 10-15% latency reduction through reduced global memory I/O.

**Timeline**: April 2025 â€“ September 2025 (6 months)

### Key Objectives

1. **Understand** nano-vLLM and vLLM architectures
   - PagedAttention scheduling mechanisms
   - Memory management strategies
   - Multi-task performance characteristics

2. **Implement** RMSNorm+Linear fusion kernel
   - Design in Triton for GPU optimization
   - Integrate into model layers
   - Validate numerical correctness

3. **Validate & Profile** performance improvements
   - Use Nsight Systems for detailed analysis
   - Benchmark across different sequence lengths and batch sizes
   - Document performance gains and insights

## ğŸ“ Project Structure

```
â”œâ”€â”€ IMPLEMENTATION_PLAN.md          # Detailed phase-by-phase plan
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ docs/                           # Learning & analysis documents
â”‚   â”œâ”€â”€ 00_README.md               # Documentation index
â”‚   â”œâ”€â”€ 01_nano_vllm_architecture.md
â”‚   â”œâ”€â”€ 02_memory_management.md
â”‚   â”œâ”€â”€ 03_attention_analysis.md
â”‚   â”œâ”€â”€ 04_baseline_metrics.md
â”‚   â”œâ”€â”€ 05_nano_vs_vllm_comparison.md
â”‚   â”œâ”€â”€ 06_fusion_design.md
â”‚   â”œâ”€â”€ 07_kernel_optimization.md
â”‚   â”œâ”€â”€ 08_validation_report.md
â”‚   â”œâ”€â”€ 09_performance_analysis.md
â”‚   â”œâ”€â”€ 10_benchmark_comparison.md
â”‚   â”œâ”€â”€ CHALLENGES_AND_SOLUTIONS.md
â”‚   â””â”€â”€ IMPLEMENTATION_GUIDE.md
â”œâ”€â”€ nanovllm/                       # Modified nano-vLLM
â”‚   â”œâ”€â”€ kernels/
â”‚   â”‚   â””â”€â”€ rms_norm_linear_fusion.py
â”‚   â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ benchmarks/                     # Performance benchmarks
â”‚   â”œâ”€â”€ bench_fusion_operator.py
â”‚   â””â”€â”€ bench_results/
â”œâ”€â”€ tests/                          # Unit tests
â”‚   â”œâ”€â”€ test_fusion_kernel.py
â”‚   â””â”€â”€ test_integration.py
â””â”€â”€ examples/                       # Usage examples
    â”œâ”€â”€ fusion_inference.py
    â””â”€â”€ baseline_inference.py
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
cd nano-vllm-coconut
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install torch triton numpy pandas matplotlib
```

### 2. Reproduce nano-vLLM

```bash
# Copy nano-vLLM code
cp -r ../nano-vllm/nanovllm ./nanovllm
cp -r ../nano-vllm/bench.py ./

# Run baseline
python example.py
```

### 3. Follow Implementation Plan

See [IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md) for detailed phases:

- **Phase 1 (Weeks 1-6)**: Source code analysis
- **Phase 2 (Weeks 7-14)**: Implement fusion kernel
- **Phase 3 (Weeks 15-18)**: Profile and optimize
- **Phase 4 (Weeks 19-22)**: Document findings
- **Phase 5 (Weeks 23-26)**: Polish and publish

## ğŸ“š Learning Resources

### nano-vLLM
- Original repository: [/nano-vllm](../nano-vllm)
- Key files: `nanovllm/llm.py`, `nanovllm/engine/`, `nanovllm/layers/`

### vLLM (for comparison)
- Original repository: [/vllm](../vllm)
- PagedAttention: `vllm/attention/`
- Memory manager: `vllm/engine/memory_controller.py`

### Triton Kernels
- [Triton Documentation](https://triton-lang.org/)
- [Triton Examples](https://github.com/openai/triton/tree/main/python/examples)
- Key concepts: block-level parallelism, warp-level operations, memory coalescing

## ğŸ“Š Success Criteria

| Phase | Deliverable | Status |
|-------|-------------|--------|
| Phase 1 | 5 analysis documents + baselines | â³ Pending |
| Phase 2 | RMSNorm+Linear fusion kernel | â³ Pending |
| Phase 3 | Performance validation (10-15% gain) | â³ Pending |
| Phase 4 | Comprehensive documentation | â³ Pending |
| Phase 5 | Production-ready code & repo | â³ Pending |

## ğŸ” Key Concepts to Master

### 1. Memory Management in LLMs
- **KV Cache**: Key-value cache storage and management
- **Paging**: Block-based memory allocation (nano-vLLM vs vLLM)
- **Memory Bandwidth**: How to minimize data movement

### 2. PagedAttention
- Token-to-block mapping
- Memory layout and access patterns
- Scheduling constraints

### 3. Kernel Optimization
- **RMSNorm**: Layer normalization variant
- **Fusion**: Combining separate operations to reduce memory I/O
- **Triton Programming**: GPU programming in Python-like syntax

### 4. Performance Profiling
- **Nsight Systems**: Timeline visualization and bottleneck analysis
- **Metrics**: Latency, throughput, memory bandwidth, utilization

## ğŸ’¡ Implementation Highlights

### RMSNorm+Linear Fusion Strategy

**Goal**: Reduce memory I/O by fusing two commonly paired operations

```
Before (separate operations):
  x --â†’ RMSNorm --â†’ y (write to global memory)
  y --â†’ Linear --â†’ z (read from global memory)

After (fused kernel):
  x --â†’ RMSNorm+Linear --â†’ z (minimal global memory I/O)
```

**Expected gains**:
- Reduce intermediate tensor write/read: ~30% memory I/O reduction
- Target latency improvement: 10-15% (hardware dependent)

## ğŸ“ Progress Tracking

Use this checklist to track overall progress:

- [ ] **Phase 1**: All 5 analysis documents completed
- [ ] **Phase 2**: Fusion kernel implemented and validated
- [ ] **Phase 3**: Performance targets achieved and profiled
- [ ] **Phase 4**: Documentation complete and polished
- [ ] **Phase 5**: Repository ready for publication

## ğŸ¤ Contributing

This is a research project. If you discover improvements or alternative approaches:

1. Document your findings in `docs/`
2. Add test cases if implementing new features
3. Update `docs/CHALLENGES_AND_SOLUTIONS.md` with learnings
4. Commit with clear messages

## ğŸ“„ License

See LICENSE file (inherited from nano-vLLM and vLLM)

## ğŸ”— References

- nano-vLLM Repository: https://github.com/...
- vLLM Repository: https://github.com/lm-sys/vllm
- Triton: https://triton-lang.org/
- PagedAttention Paper: [PagedAttention paper link]

---

**Status**: Starting Phase 1 (January 2026)  
**Last Updated**: January 6, 2026
