# nano-vLLM Optimization: RMSNorm and Linear Fusion Research

A research project implementing and optimizing the nano-vLLM lightweight inference framework with Triton-based kernel fusion.

## Project Overview

This project reproduces the nano-vLLM lightweight LLM inference framework and implements RMSNorm and Linear operator fusion using Triton to achieve 10-15% latency reduction through reduced global memory I/O.

### Key Objectives

1. Understand nano-vLLM and vLLM architectures
   - PagedAttention scheduling mechanisms
   - Memory management strategies
   - Multi-task performance characteristics

2. Implement RMSNorm and Linear fusion kernel
   - Design in Triton for GPU optimization
   - Integrate into model layers
   - Validate numerical correctness

3. Validate and profile performance improvements
   - Use Nsight Systems for detailed analysis
   - Benchmark across different sequence lengths and batch sizes
   - Document performance gains and insights

## Project Structure

```
â”œâ”€â”€ implementation_plan.md          # Detailed phase-by-phase plan
â”œâ”€â”€ README.md                       # Project overview
â”œâ”€â”€ docs/                           # Research and analysis documents
â”‚   â”œâ”€â”€ 00_README.md               # Documentation index
â”‚   â”œâ”€â”€ 01_architecture.md
â”‚   â”œâ”€â”€ 02_memory_management.md
â”‚   â”œâ”€â”€ 03_attention.md
â”‚   â”œâ”€â”€ 04_baseline.md
â”‚   â”œâ”€â”€ 05_comparison.md
â”‚   â”œâ”€â”€ 06_fusion_design.md
â”‚   â”œâ”€â”€ 07_kernel_optimization.md
â”‚   â”œâ”€â”€ 08_validation.md
â”‚   â”œâ”€â”€ 09_performance.md
â”‚   â”œâ”€â”€ 10_benchmarks.md
â”‚   â”œâ”€â”€ challenges.md
â”‚   â””â”€â”€ implementation_guide.md
â”œâ”€â”€ nanovllm/                       # Modified nano-vLLM
â”‚   â”œâ”€â”€ kernels/
â”‚   â”‚   â””â”€â”€ rms_norm_linear_fusion.py
â”‚   â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ benchmarks/                     # Performance benchmarks
â”‚   â”œâ”€â”€ bench_fusion_operator.py
â”‚   â””â”€â”€ bench_results/
â”œâ”€â”€ tests/                          # Unit tests
â”‚   â”œâ”€â”€ test_fusion_kernel.py
â”‚   â””â”€â”€ test_integration.py
â””â”€â”€ examples/                       # Usage examples
    â”œâ”€â”€ fusion_inference.py
    â””â”€â”€ baseline_inference.py
```

## Environment Setup

```bash
cd nano-vllm-coconut
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

pip install torch triton numpy pandas matplotlib
```

### Implementation Phases

See [implementation_plan.md](implementation_plan.md) for detailed phases:

- Phase 1 (Weeks 1-6): Source code analysis
- Phase 2 (Weeks 7-14): Implement fusion kernel
- Phase 3 (Weeks 15-18): Profile and optimize
- Phase 4 (Weeks 19-22): Document findings
- Phase 5 (Weeks 23-26): Polish and publish

## Technical Background

### nano-vLLM
- Original repository: nano-vllm
- Key files: `nanovllm/llm.py`, `nanovllm/engine/`, `nanovllm/layers/`

### vLLM (for comparison)
- Original repository: vllm
- PagedAttention: `vllm/attention/`
- Memory manager: `vllm/engine/memory_controller.py`

### Triton Kernels
- Triton Documentation: https://triton-lang.org/
- Key concepts: block-level parallelism, warp-level operations, memory coalescing

## Research Objectives

| Phase | Deliverable | 
|-------|-------------|
| Phase 1 | Analysis documents and baselines |
| Phase 2 | RMSNorm and Linear fusion kernel |
| Phase 3 | Performance validation |
| Phase 4 | Comprehensive documentation |
| Phase 5 | Production-ready implementation |

## Key Technical Concepts

### 1. Memory Management in LLMs
- KV Cache: Key-value cache storage and management
- Paging: Block-based memory allocation
- Memory Bandwidth: Minimizing data movement

### 2. PagedAttention
- Token-to-block mapping
- Memory layout and access patterns
- Scheduling constraints

### 3. Kernel Optimization
- **RMSNorm**: Layer normalization variant
- **Fusion**: Combining separate operations to reduce memory I/O
- **Triton Programming**: GPU programming in Python-like syntax

### 4. Performance Profiling
- **Nsight Systems**: Timeline visualization and bottleneck analysis
- **Metrics**: Latency, throughput, memory bandwidth, utilization

## ðŸ’¡ Implementation Highlights

### RMSNorm+Linear Fusion Strategy

**Goal**: Reduce memory I/O by fusing two commonly paired operations

```
Before (separate operations):
  x --â†’ RMSNorm --â†’ y (write to global memory)
  y --â†’ Linear --â†’ z (read from global memory)

After (fused kernel):
  x â†’ RMSNorm and Linear â†’ z (minimal global memory I/O)
```

Expected performance gains:
- Reduce intermediate tensor write/read: approximately 30% memory I/O reduction
- Target latency improvement: 10-15% (hardware dependent)

## References

- nano-vLLM Repository: https://github.com/nanovllm/nano-vllm
- vLLM Repository: https://github.com/vllm-project/vllm
- Triton: https://triton-lang.org/
- PagedAttention Paper: Efficient Memory Management for Large Language Model Serving with PagedAttention
