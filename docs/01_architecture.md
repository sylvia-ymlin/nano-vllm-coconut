# 1. nano-vLLM Architecture Analysis

**Phase**: 1 (Weeks 1-2)  
**Status**: ðŸ”§ In Progress  
**Last Updated**: January 6, 2026

## Overview

This document provides a comprehensive analysis of the nano-vLLM lightweight LLM inference framework architecture. We examine the project structure, design philosophy, core components, and execution flow to establish a foundation for understanding the system.

**Key Finding**: nano-vLLM is a **minimal reimplementation of vLLM** (~1,200 lines of Python) with comparable performance but much simpler codebase.

## Sections to Complete

### 1.1 Project Design Philosophy
- [x] Lightweight vs full-featured frameworks - **DONE**
- [x] Design trade-offs - **DONE**
- [x] Target use cases - **DONE**
- [x] Key design decisions - **DONE**

### 1.2 Directory Structure Analysis
- [x] `/nanovllm/` core module organization - **DONE**
- [x] `/engine/` - execution engine - **DONE**
- [x] `/layers/` - neural network layers - **IN PROGRESS**
- [ ] `/models/` - model implementations
- [ ] `/utils/` - utilities
- [ ] `/benchmarks/` - performance evaluation

### 1.3 Core Components
- [x] `llm.py` - main inference interface - **DONE**
- [x] `config.py` - configuration management - **IN PROGRESS**
- [x] `sampling_params.py` - sampling parameters - **DONE**
- [ ] Integration between components

### 1.4 Execution Flow
- [ ] Model loading pipeline
- [ ] Token generation process
- [ ] Batch processing
- [ ] KV cache management
- [ ] Attention computation

### 1.5 Comparison with vLLM
- [ ] Architectural differences
- [ ] Feature parity
- [ ] Performance characteristics
- [ ] Size and complexity metrics

## Key Files to Study

```
nano-vllm/
â”œâ”€â”€ nanovllm/
â”‚   â”œâ”€â”€ __init__.py              # Package initialization (exports LLM, SamplingParams)
â”‚   â”œâ”€â”€ llm.py                   # Main inference class â­ Entry point
â”‚   â”œâ”€â”€ config.py                # Configuration management â­
â”‚   â”œâ”€â”€ sampling_params.py        # Sampling parameters
â”‚   â”œâ”€â”€ engine/                   # Core inference engine â­
â”‚   â”‚   â”œâ”€â”€ llm_engine.py        # Main engine (request management, scheduling)
â”‚   â”‚   â”œâ”€â”€ model_runner.py      # Model execution (forward pass)
â”‚   â”‚   â”œâ”€â”€ scheduler.py         # Request scheduling & batching
â”‚   â”‚   â”œâ”€â”€ sequence.py          # Token sequence management
â”‚   â”‚   â””â”€â”€ block_manager.py     # KV cache block allocation
â”‚   â”œâ”€â”€ layers/                   # Layer implementations â­
â”‚   â”‚   â”œâ”€â”€ attention.py         # Attention with KV cache management
â”‚   â”‚   â”œâ”€â”€ layernorm.py         # RMSNorm implementation
â”‚   â”‚   â”œâ”€â”€ linear.py            # Linear layers (Q, K, V, output projections)
â”‚   â”‚   â”œâ”€â”€ rotary_embedding.py  # RoPE positional encoding
â”‚   â”‚   â”œâ”€â”€ activation.py        # Activation functions (SwiGLU)
â”‚   â”‚   â”œâ”€â”€ embed_head.py        # Embedding and output head
â”‚   â”‚   â””â”€â”€ sampler.py           # Token sampling
â”‚   â”œâ”€â”€ models/                   # Model definitions (Qwen, LLaMA, etc.)
â”‚   â””â”€â”€ utils/                    # Utilities
â”œâ”€â”€ example.py                   # Usage example â­
â”œâ”€â”€ bench.py                     # Benchmarking script
â””â”€â”€ README.md                    # Project overview
```

**High Priority** (â­): Start with these files for core understanding

## Findings

### Architecture Overview

**nano-vLLM is a reimplementation-from-scratch approach** to vLLM, not a fork. Key characteristics:

1. **Scale**: ~1,200 lines of Python code (vs vLLM's ~100K+ lines)
2. **Philosophy**: Readable, educational codebase with comparable performance
3. **Performance**: Achieves 1434.13 tokens/s vs vLLM's 1361.84 tokens/s on Qwen3-0.6B
4. **Model Support**: Originally designed for smaller models (Qwen3-0.6B tested)

**Package Structure**:
```
nanovllm/
â”œâ”€â”€ llm.py ..................... Main user-facing class (inherits from LLMEngine)
â”œâ”€â”€ config.py .................. Configuration management
â”œâ”€â”€ sampling_params.py ......... Sampling configuration
â”œâ”€â”€ engine/ .................... Core inference engine
â”‚   â”œâ”€â”€ llm_engine.py ......... Main engine implementation
â”‚   â”œâ”€â”€ model_runner.py ....... Model execution (forward pass)
â”‚   â”œâ”€â”€ scheduler.py .......... Request scheduling & batching
â”‚   â”œâ”€â”€ sequence.py ........... Token sequence management
â”‚   â””â”€â”€ block_manager.py ...... KV cache block allocation
â”œâ”€â”€ layers/ .................... Neural network layers (optimized kernels)
â”œâ”€â”€ models/ .................... Model-specific implementations
â””â”€â”€ utils/ ..................... Utility functions
```

### Design Principles

**1. Minimalism**: Only essential components for inference
- No training support
- No distributed training features
- No advanced distributed serving features (yet)

**2. Readability**: Educational codebase for learning
- Clear function names and structure
- Minimal abstraction layers
- Direct implementation of core concepts

**3. Performance**: Without sacrificing simplicity
- Uses CUDA kernels where critical
- Memory-efficient KV cache management
- Efficient scheduling and batching

**4. Compatibility**: vLLM-like API
- Similar `LLM` class interface
- Similar `SamplingParams`
- Drop-in replacement for small models

### Performance Characteristics

**Tested Configuration**:
- Hardware: RTX 4070 Laptop (8GB VRAM)
- Model: Qwen3-0.6B (very small model)
- Test: 256 sequences, 100-1024 token input/output

**Results**:
| Engine | Output Tokens | Time | Throughput |
|--------|--------------|------|-----------|
| vLLM | 133,966 | 98.37s | 1361.84 tok/s |
| nano-vLLM | 133,966 | 93.41s | 1434.13 tok/s |

**Why faster**: Simplified implementation overhead, less memory management complexity

### Limitations and Opportunities

**Current Limitations**:
1. Tested only on small models (0.6B parameters)
2. No async/concurrent request handling (likely)
3. Simpler scheduler than vLLM
4. No tensor parallelism (in current version)
5. Fewer optimization options

**Optimization Opportunities**:
1. **RMSNorm+Linear Fusion**: Paired operations in attention/MLP layers (our target!)
2. **Prefix Caching**: Reuse computed KV cache for repeated prefixes
3. **Attention Optimization**: More efficient attention implementations
4. **Memory Layout**: Better KV cache organization
5. **Quantization Support**: Reduce model size

## Key Insights

*Key learnings from initial analysis*

1. **Simplicity is Powerful**: nano-vLLM achieves better throughput than vLLM with 1/100th the code complexity
2. **Clean Architecture**: Separation of concerns (engine, scheduler, model_runner, block_manager) makes code understandable
3. **KV Cache is Critical**: Block manager and sequence management are central to performance
4. **Small Models Ready**: Designed and tested for small models (0.6B-13B range)
5. **Fusion Opportunity Clear**: Many RMSNorm+Linear pairs likely exist in attention/MLP blocks - ideal for our optimization 

## Code Annotations

### Execution Flow Diagram

```
User Call: llm.generate(prompts, sampling_params)
    â†“
LLMEngine.__init__
    â”œâ”€ Load tokenizer
    â”œâ”€ Create ModelRunner (main GPU process + parallel processes for tensor parallelism)
    â””â”€ Initialize Scheduler (manages request queue and batching)
    â†“
LLMEngine.add_request (for each prompt)
    â”œâ”€ Tokenize prompt â†’ token_ids
    â””â”€ Create Sequence object â†’ Add to scheduler
    â†“
LLMEngine.step (repeatedly until all requests done)
    â”œâ”€ Scheduler.schedule()
    â”‚   â””â”€ Return: (seqs, is_prefill flag)
    â”‚       - is_prefill: True for first step (prefill phase)
    â”‚       - is_prefill: False for subsequent steps (decode phase)
    â”‚
    â”œâ”€ ModelRunner.run(seqs, is_prefill)
    â”‚   â”œâ”€ For each sequence:
    â”‚   â”‚   â”œâ”€ Embed tokens (embed_head.py)
    â”‚   â”‚   â””â”€ For each layer in model:
    â”‚   â”‚       â”œâ”€ RMSNorm (layernorm.py) â† target for fusion!
    â”‚   â”‚       â”œâ”€ Attention (attention.py)
    â”‚   â”‚       â”‚   â”œâ”€ Project to Q, K, V (linear.py) â† target for fusion!
    â”‚   â”‚       â”‚   â”œâ”€ Update KV cache (block_manager.py)
    â”‚   â”‚       â”‚   â””â”€ FlashAttention (from flash-attn)
    â”‚   â”‚       â”œâ”€ RMSNorm (residual) â† target for fusion!
    â”‚   â”‚       â””â”€ MLP (attention.py)
    â”‚   â”‚           â”œâ”€ RMSNorm (layernorm.py) â† target for fusion!
    â”‚   â”‚           â”œâ”€ Linear gate projection (linear.py) â† target for fusion!
    â”‚   â”‚           â”œâ”€ Activation (SwiGLU)
    â”‚   â”‚           â””â”€ Linear output projection (linear.py)
    â”‚   â”‚
    â”‚   â”œâ”€ Output embedding (embed_head.py)
    â”‚   â””â”€ Sample next token (sampler.py)
    â”‚
    â””â”€ Update sequences with new tokens
    â†“
LLMEngine.generate (yields results as sequences complete)
```

### Key Layer Components

**RMSNorm Layer** (`layernorm.py`):
```python
@torch.compile
def rms_forward(x: torch.Tensor) -> torch.Tensor:
    """
    Compute: x / sqrt(mean(x^2) + eps) * weight
    
    This is RMS Layer Normalization.
    Often followed immediately by a Linear layer.
    """
    orig_dtype = x.dtype
    x = x.float()
    var = x.pow(2).mean(dim=-1, keepdim=True)
    x.mul_(torch.rsqrt(var + self.eps))
    x = x.to(orig_dtype).mul_(self.weight)
    return x
```

**Linear Layer** (`linear.py`):
```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    """
    Compute: x @ weight.T + bias
    
    Used for:
    - Q, K, V projections in attention
    - MLP hiddenâ†’output
    """
```

### Model Architecture Pattern

All models follow this pattern (from models/ directory):

```
Model:
    for each token position:
        Embed(token)
        for each TransformerBlock:
            â”Œâ”€ Layer Norm
            â”œâ”€ Attention
            â””â”€ (residual connection)
            â”Œâ”€ Layer Norm
            â”œâ”€ MLP
            â”‚   â”œâ”€ Linear (expand to 4x)
            â”‚   â”œâ”€ Activation (SwiGLU)
            â”‚   â””â”€ Linear (project back)
            â””â”€ (residual connection)
        Output projection
        Sample next token
```

**Fusion Opportunities**:
- Layer Norm â†’ Attention Q/K/V projections: **2-3 pairs per layer**
- Layer Norm â†’ MLP gate projection: **1 pair per layer**
- For 32-layer model: **96-128 RMSNorm+Linear pairs total**

## Questions for Investigation

- [x] How is the KV cache managed across requests? â†’ **Block manager handles it**
- [x] What scheduling algorithm is used? â†’ **Custom scheduler (need to examine scheduler.py)**
- [x] How does memory allocation work? â†’ **Block allocation for KV cache (block_manager.py)**
- [x] What are the bottlenecks in current implementation? â†’ **RMSNorm+Linear pairs likely candidates**
- [x] How are attention operations implemented? â†’ **Uses FlashAttention library with custom KV cache storage**
- [x] What parallelization strategies are used? â†’ **Tensor parallelism via multiprocessing, no data parallelism visible yet**

**To Investigate Further**:
- [ ] Exact scheduler implementation and batching strategy
- [ ] KV cache memory overhead
- [ ] Impact of is_prefill flag on computation
- [ ] Exact transformer block implementation in models/

## References

### Internal
- [README.md](../README.md) - Project overview
- [IMPLEMENTATION_PLAN.md](../IMPLEMENTATION_PLAN.md) - Full project plan

### External
- nano-vLLM repository: [link]
- Related papers: [links]

## Next Steps

1. Read nano-vLLM README and documentation
2. Examine file structure and import hierarchy
3. Trace execution flow from example.py
4. Compare with vLLM equivalent components
5. Document findings in this file

---

**Estimated Duration**: 2 weeks  
**Difficulty**: Medium  
**Dependencies**: None (foundation work)
